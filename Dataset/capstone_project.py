# -*- coding: utf-8 -*-
"""Capstone Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yRDNSDqv8268L04RwOZXAcwQCcCTFHIF
"""

!pip install xgboost

# Import semua library yang dibutuhkan
#Untuk Data
import numpy as np
import pandas as pd

#untuk Visualisasi
import matplotlib.pyplot as plt
import seaborn as sns

#Untuk kebutuhan proyek machine learning
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, mean_squared_error, r2_score
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import silhouette_score
from sklearn.cluster import KMeans
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.preprocessing import OneHotEncoder, StandardScaler, KBinsDiscretizer, MinMaxScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from imblearn.over_sampling import SMOTE
from collections import Counter
from yellowbrick.cluster import KElbowVisualizer
#Untuk ekspor data dan penyimpanan model
import joblib

#Loading data dan Menampilkan data pada 5 baris pertama
df = pd.read_csv("Dataset_pertanian_dengan_pupuk.csv")
df.head()

# Menampilkan jumlah baris kolom dan jenis data dalam dataset dengan info.
df.info()

# Menampilkan statistik deskriptif dataset dengan menjalankan describe
df.describe(include="all")

# Mengecek dataset menggunakan isnull().sum()
missing_values = df.isnull().sum()
missing_values [missing_values > 0]

# Mengecek dataset menggunakan duplicated().sum()
jumlah_duplikat = df.duplicated().sum()
print("Jumlah data duplikat:", jumlah_duplikat)

import seaborn as sns
import matplotlib.pyplot as plt

numeric_features = df.select_dtypes(include='number').columns
# Boxplot sebelum scaling
for feature in numeric_features:
    plt.figure(figsize=(6, 3))
    sns.boxplot(x=df[feature])
    plt.title(f'Box Plot of {feature}')
    plt.show()

#Penanganan Outliers
# membersihkan kategori supaya konsisten
df["Commodity"] = df["Commodity"].astype(str).str.strip().str.title() #untuk kmoditas karena jika melakukan penghapusan outliers pada umumnya, terdapat komoditas yang hilang

# mengambil kolom numerik
numeric_features = df.select_dtypes(include=['float64','int64']).columns

# membuat dataframe kosong untuk menampung hasil
df_clean = pd.DataFrame()

# Loop untuk setiap komoditas
for komoditas, subset in df.groupby("Commodity"):
    # Hitung IQR untuk subset (komoditas tertentu)
    Q1 = subset[numeric_features].quantile(0.25)
    Q3 = subset[numeric_features].quantile(0.75)
    IQR = Q3 - Q1

    mask = ~((subset[numeric_features] < (Q1 - 3 * IQR)) |
             (subset[numeric_features] > (Q3 + 3 * IQR))).any(axis=1)


    df_clean = pd.concat([df_clean, subset[mask]])

# Standardisasi untuk fitur numerik
numeric_features = df_clean.select_dtypes(include=['int64','float64']).columns.difference(
    ["Province","District","Commodity"]
)

# Menyalin data agar df_clean tetap utuh
df_standardized = df_clean.copy()

# Standarisasi
scaler = StandardScaler()
df_standardized[numeric_features] = scaler.fit_transform(df_standardized[numeric_features])

#Mengonversi data kategorikal
#Mengambil kolom kategorikkal
categorical_cols = ["Province", "District", "Commodity"]

# Menggunakan dataset yang sudah distandarisasi
df_encoded = pd.get_dummies(df_standardized, columns=categorical_cols, drop_first=True)

print(df_encoded.head())
print(f"Jumlah kolom setelah encoding: {df_encoded.shape[1]}")

# Menghitung jumlah variabel
numeric_cols = df_encoded.select_dtypes(include=['float64','int64']).columns

# Menentukan jumlah baris dan kolom untuk grid subplot
num_vars = len(numeric_cols)
n_cols = 3  # Jumlah kolom yang diinginkan
n_rows =-(-num_vars // n_cols)

# Membuat subplot
plt.figure(figsize=(20, n_rows * 4))


# Plot setiap variabel
for i, col in enumerate(numeric_cols, 1):
    plt.subplot(n_rows, n_cols, i)
    sns.histplot(df_encoded[col], kde=True)
    plt.title(f"Distribusi {col}")
    plt.xlabel("")
    plt.ylabel("")

# Menyesuaikan layout agar lebih rapi
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Mengurutkan distribusi provinsi
plt.figure(figsize=(10,8))
sns.countplot(y="Province", data=df, order=df["Province"].value_counts().index)
plt.title("Distribusi Provinsi (Terurut)")
plt.tight_layout()
plt.show()

# Menampilkan 20 kota dengan jumlah data terbanyak
top_districts = df["District"].value_counts().head(20)

plt.figure(figsize=(10,8))
sns.barplot(x=top_districts.values, y=top_districts.index)
plt.title("20 Kota/Kabupaten dengan Jumlah Data Terbanyak")
plt.xlabel("Jumlah")
plt.ylabel("District")
plt.tight_layout()
plt.show()

# Menampilkan semua distribusi komoditas
plt.figure(figsize=(10,5))
sns.countplot(x="Commodity", data=df, order=df["Commodity"].value_counts().index)
plt.title("Distribusi Semua Komoditas")
plt.xlabel("Komoditas")
plt.ylabel("Jumlah")
plt.xticks(rotation=45)  # miringkan label agar tidak tumpang tindih
plt.tight_layout()
plt.show()

# Visualisasi korelasi antar variabel numerik
numeric_cols = df_standardized.select_dtypes(include=['float64','int64']).columns
correlation_matrix = df_standardized[numeric_cols].corr()

plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", vmin=-1, vmax=1)
plt.title('Correlation Matrix (Fitur Numerik Asli)')
plt.show()

# Menghitung korelasi antara variabel target dan semua variabel lainnya
import matplotlib.pyplot as plt

# Hanya mengambil kolom numerik
numeric_df = df_standardized.select_dtypes(include=['float64','int64'])

target_cols = ["Init_Capital_RpHa", "Maintenance_Cost_RpHa", "Production_KgHa"]

for target in target_cols:
    target_corr = numeric_df.corr()[target]  # hitung korelasi terhadap target
    target_corr_sorted = target_corr.abs().sort_values(ascending=False)

    plt.figure(figsize=(10, 6))
    target_corr_sorted.plot(kind='bar')
    plt.title(f'Correlation with {target}')
    plt.xlabel('Variables')
    plt.ylabel('Correlation Coefficient')
    plt.tight_layout()
    plt.show()

"""Feature Engineering

"""

#Fitur interaksi
# Contoh: Menghitung rasio hasil per luas lahan
df_encoded["Yield_per_Area"] = df_encoded["Production_KgHa"] / (df_encoded["Area_Ha"] + 1e-6)

# Biaya per produksi (Cost Efficiency)
df_encoded["Cost_per_Kg"] = (df_encoded["Init_Capital_RpHa"] + df_encoded["Maintenance_Cost_RpHa"]) / (df_encoded["Production_KgHa"] + 1e-6)

# Harga pupuk rata-rata
df_encoded["Avg_Fertilizer_Price"] = (
    df_encoded["InputPrice_Urea_RpKg"] + df_encoded["InputPrice_SP36_RpKg"] + df_encoded["InputPrice_KCl_RpKg"]
) / 3

#Fitur Polinomial
# Kuadratkan variabel yang punya distribusi simetris (misalnya pH atau Curah Hujan)
df_encoded["Soil_pH_sq"] = df_encoded["Soil_pH"] ** 2
df_encoded["Rain_mm_log"] = np.log1p(df_encoded["Rain_mm"] - df_encoded["Rain_mm"].min() + 1)

#Fitur Temporal
# Jarak tahun terhadap tahun awal
df_encoded["Year_Offset"] = df_encoded["Year"] - df_encoded["Year"].min()

# Trend Produksi (perubahan tahun-ke-tahun per komoditas)
df_encoded["Production_Trend"] = df_encoded.groupby("Commodity_Padi")["Production_KgHa"].diff().fillna(0)

#Fitur Klasifikasi Berdasarkan THreshold
# Kategori kesuburan tanah berdasarkan pH
df_encoded["Soil_pH_Category"] = pd.cut(
    df_encoded["Soil_pH"],
    bins=[-np.inf, -0.5, 0.5, np.inf],
    labels=["Low", "Optimal", "High"]
)

# Kategori curah hujan
df_encoded["Rain_Category"] = pd.qcut(df_encoded["Rain_mm"], q=4, labels=["Very Low", "Low", "Medium", "High"])

"""Data Splitting"""

# Tentukan target kolom
target_cols = ["Init_Capital_RpHa", "Maintenance_Cost_RpHa", "Production_KgHa"]

# Pisahkan X (fitur) dan y (target)
X = df_encoded.drop(columns=target_cols)
y = df_encoded[target_cols]

#One-Hot Encoding untuk kolom kategorikal
categorical_cols = X.select_dtypes(include=['object', 'category']).columns
X_encoded = pd.get_dummies(X, columns=categorical_cols, drop_first=True)

# membaagi dataset menjadi train (80%) dan test (20%)
X_train, X_test, y_train, y_test = train_test_split(
    X_encoded, y, test_size=0.2, random_state=42
)

print("Ukuran dataset:")
print(f"X_train: {X_train.shape}")
print(f"X_test: {X_test.shape}")
print(f"y_train: {y_train.shape}")
print(f"y_test: {y_test.shape}")

"""Training Model"""

#Training RandomForestRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_squared_error
import numpy as np

rf_models = {}
rf_results = {}

for col in target_cols:
    model = RandomForestRegressor(n_estimators=200, random_state=42)
    model.fit(X_train, y_train[col])
    y_pred = model.predict(X_test)

    rf_models[col] = model
    rf_results[col] = {
        "R2": r2_score(y_test[col], y_pred),
        "RMSE": np.sqrt(mean_squared_error(y_test[col], y_pred))  # hitung manual
    }

print("Hasil RandomForestRegressor")
for col, res in rf_results.items():
    print(f"{col}: R² = {res['R2']:.3f}, RMSE = {res['RMSE']:.3f}")

#Training dan evaluasi XGBoost
from xgboost import XGBRegressor

xgb_models = {}
xgb_results = {}

for col in target_cols:
    model = XGBRegressor(
        n_estimators=300,
        learning_rate=0.05,
        max_depth=6,
        random_state=42
    )
    model.fit(X_train, y_train[col])
    y_pred = model.predict(X_test)

    xgb_models[col] = model
    xgb_results[col] = {
        "R2": r2_score(y_test[col], y_pred),
        "RMSE": np.sqrt(mean_squared_error(y_test[col], y_pred))
    }

print("Hasil XGBoostRegressor")
for col, res in xgb_results.items():
    print(f"{col}: R² = {res['R2']:.3f}, RMSE = {res['RMSE']:.3f}")

#Membandingkan performa kedua model
compare_df = pd.DataFrame({
    "RandomForest_R2": {k: v["R2"] for k, v in rf_results.items()},
    "XGBoost_R2": {k: v["R2"] for k, v in xgb_results.items()},
    "RandomForest_RMSE": {k: v["RMSE"] for k, v in rf_results.items()},
    "XGBoost_RMSE": {k: v["RMSE"] for k, v in xgb_results.items()},
})

print(compare_df)

ensemble_results = {}
for col in target_cols:
    rf_pred = rf_models[col].predict(X_test)
    xgb_pred = xgb_models[col].predict(X_test)
    ensemble_pred = (rf_pred + xgb_pred) / 2

    ensemble_results[col] = {
        "R2": r2_score(y_test[col], ensemble_pred),
        "RMSE": np.sqrt(mean_squared_error(y_test[col], ensemble_pred))
    }

ensemble_df = pd.DataFrame(ensemble_results).T
print(ensemble_df)

import joblib

for col in target_cols:
    joblib.dump(rf_models[col], f'rf_baseline_{col}.pkl')
    joblib.dump(xgb_models[col], f'xgb_baseline_{col}.pkl')

print("Semua model baseline berhasil disimpan!")

"""Hyperparameter"""

from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestRegressor
import numpy as np

# Definisikan parameter yang akan di-tuning
rf_param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 5, 10, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['sqrt', 'log2', None]
}

rf_best_models = {}
rf_best_params = {}
rf_best_scores = {}

# Target yang ingin diprediksi
target_cols = ['Production_KgHa', 'Init_Capital_RpHa', 'Maintenance_Cost_RpHa']

for col in target_cols:
    print(f"\n Hyperparameter tuning RandomForest untuk target: {col}")
    rf_random_search = RandomizedSearchCV(
        estimator=RandomForestRegressor(random_state=42),
        param_distributions=rf_param_grid,
        n_iter=5,
        cv=3,
        scoring='r2',
        random_state=42,
        n_jobs=-1,
        verbose=1
    )
    rf_random_search.fit(X_train, y_train[col])
    rf_best_models[col] = rf_random_search.best_estimator_
    rf_best_params[col] = rf_random_search.best_params_
    rf_best_scores[col] = rf_random_search.best_score_

    print(f"Best Parameters ({col}): {rf_random_search.best_params_}")
    print(f"Best R² (CV) ({col}): {rf_random_search.best_score_:.3f}")

# Melatih ulang semua model terbaik Random Forest untuk tiap target
for col in target_cols:
    print(f"Melatih ulang model terbaik Random Forest untuk target: {col}")
    rf_best_models[col].fit(X_train, y_train[col])

from sklearn.model_selection import RandomizedSearchCV
from xgboost import XGBRegressor
import numpy as np
import pandas as pd
import joblib

#  Definisikan parameter grid untuk XGBoost

xgb_param_grid = {
    'n_estimators': [100, 200, 300],          # jumlah pohon
    'learning_rate': [0.01, 0.05, 0.1],       # laju pembelajaran
    'max_depth': [3, 5, 7],                   # kedalaman pohon
    'subsample': [0.6, 0.8, 1.0],             # proporsi data yang dipakai tiap pohon
    'colsample_bytree': [0.6, 0.8, 1.0],      # proporsi fitur tiap pohon
    'gamma': [0, 0.1, 0.3]                    # regularisasi
}

# Inisialisasi dictionary untuk menyimpan hasil tuning

xgb_best_models = {}
xgb_best_params = {}
xgb_best_scores = {}

# Tentukan kolom target yang ingin diprediksi

print("Kolom target yang tersedia di y_train:", list(y_train.columns))

target_cols = ['Production_KgHa', 'Init_Capital_RpHa', 'Maintenance_Cost_RpHa']

#  Looping untuk tuning masing-masing target

for col in target_cols:
    if col not in y_train.columns:
        print(f" Kolom '{col}' tidak ditemukan di y_train, dilewati...")
        continue

    print(f"\n Hyperparameter tuning XGBoost untuk target: {col}")

    # Setup RandomizedSearch
    xgb_random_search = RandomizedSearchCV(
        estimator=XGBRegressor(
            random_state=42,
            objective='reg:squarederror',
            tree_method='hist'
        ),
        param_distributions=xgb_param_grid,
        n_iter=5,
        cv=3,
        scoring='r2',       # evaluasi berdasarkan R²
        random_state=42,
        n_jobs=-1,
        verbose=1
    )

    # Jalankan tuning
    xgb_random_search.fit(X_train, y_train[col])

    # Simpan hasil terbaik
    xgb_best_models[col] = xgb_random_search.best_estimator_
    xgb_best_params[col] = xgb_random_search.best_params_
    xgb_best_scores[col] = xgb_random_search.best_score_

    print(f" Best Parameters ({col}): {xgb_random_search.best_params_}")
    print(f" Best R² (CV) ({col}): {xgb_random_search.best_score_:.3f}")

# Melatih ulang semua model hasil tuning XGBoost
for col in target_cols:
    print(f"Melatih ulang model XGBoost terbaik untuk target: {col}")
    xgb_best_models[col].fit(X_train, y_train[col])

from sklearn.metrics import mean_squared_error, r2_score
import numpy as np
import pandas as pd

results = []  # untuk menyimpan semua hasil evaluasi

print("Evaluasi Model untuk Semua Target\n")

for col in target_cols:
    print(f"Evaluasi Model untuk Target: {col}")

    # Prediksi Random Forest
    y_pred_rf = rf_best_models[col].predict(X_test)
    mse_rf = mean_squared_error(y_test[col], y_pred_rf)
    rmse_rf = np.sqrt(mse_rf)
    r2_rf = r2_score(y_test[col], y_pred_rf)

    # Prediksi XGBoost
    y_pred_xgb = xgb_best_models[col].predict(X_test)
    mse_xgb = mean_squared_error(y_test[col], y_pred_xgb)
    rmse_xgb = np.sqrt(mse_xgb)
    r2_xgb = r2_score(y_test[col], y_pred_xgb)

    # Cetak hasil per target
    print(f"   Random Forest → MSE: {mse_rf:.4f}, RMSE: {rmse_rf:.4f}, R²: {r2_rf:.4f}")
    print(f"   XGBoost       → MSE: {mse_xgb:.4f}, RMSE: {rmse_xgb:.4f}, R²: {r2_xgb:.4f}\n")

    # Simpan hasil ke list
    results.append({
        "Target": col,
        "RF_R2": r2_rf,
        "RF_RMSE": rmse_rf,
        "XGB_R2": r2_xgb,
        "XGB_RMSE": rmse_xgb
    })

# Buat DataFrame hasil
results_df = pd.DataFrame(results)

print("\nRingkasan Hasil Evaluasi:")
print(results_df.round(4))

# Simpan ke file CSV
results_df.to_csv("model_evaluation_results.csv", index=False)
print("\nHasil evaluasi disimpan dalam file 'model_evaluation_results.csv'")

import joblib

# Simpan semua model terbaik untuk tiap target
for col in target_cols:
    nama_file_rf = f"random_forest_model_{col}.pkl"
    nama_file_xgb = f"xgboost_model_{col}.pkl"

    joblib.dump(rf_best_models[col], nama_file_rf)
    joblib.dump(xgb_best_models[col], nama_file_xgb)

    print(f"Model Random Forest untuk {col} telah disimpan sebagai: {nama_file_rf}")
    print(f"Model XGBoost untuk {col} telah disimpan sebagai: {nama_file_xgb}\n")

"""Model Rekomendasi"""

# Import library yang dibutuhkan untuk model rekomendasi
from sklearn.neighbors import KNeighborsRegressor
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
import joblib

# Pilih fitur yang relevan untuk rekomendasi pupuk
# Fitur-fitur ini paling berpengaruh terhadap jenis dan jumlah pupuk yang dibutuhkan
features_rekomendasi = [
    'Soil_N_index', 'Soil_P_index', 'Soil_K_index',
    'Soil_pH', 'Temp_C', 'Rain_mm', 'Humidity_pct'
]

# Tentukan variabel target (jenis pupuk)
targets_rekomendasi = ['Pupuk_Urea_kgHa', 'Pupuk_SP36_kgHa', 'Pupuk_KCl_kgHa']

# Pisahkan data fitur dan target
X_rekomendasi = df[features_rekomendasi]
y_rekomendasi = df[targets_rekomendasi]

# Buat pipeline untuk preprocessing dan model
# StandardScaler akan menormalisasi data, dan KNeighborsRegressor akan mencari "tetangga" terdekat
# untuk memberikan rekomendasi.
pipeline_rekomendasi = Pipeline([
    ('scaler', StandardScaler()),
    ('knn', KNeighborsRegressor(n_neighbors=5)) # n_neighbors=5 berarti model akan mencari 5 data paling mirip
])

# Latih model
print("Melatih model rekomendasi pupuk...")
pipeline_rekomendasi.fit(X_rekomendasi, y_rekomendasi)
print("Model rekomendasi pupuk berhasil dilatih.")

# Simpan model yang sudah dilatih ke dalam file
nama_file_rekomendasi = 'model_rekomendasi_pupuk.pkl'
joblib.dump(pipeline_rekomendasi, nama_file_rekomendasi)

print(f"Model rekomendasi pupuk telah disimpan dalam file: {nama_file_rekomendasi}")

# Contoh cara menggunakan model untuk prediksi (rekomendasi)
contoh_input = pd.DataFrame({
    'Soil_N_index': [3],
    'Soil_P_index': [2],
    'Soil_K_index': [3],
    'Soil_pH': [5.5],
    'Temp_C': [27],
    'Rain_mm': [2000],
    'Humidity_pct': [80]
})

rekomendasi_pupuk = pipeline_rekomendasi.predict(contoh_input)
print("\nContoh Rekomendasi:")
print(f"Pupuk Urea: {rekomendasi_pupuk[0][0]:.2f} kg/Ha")
print(f"Pupuk SP36: {rekomendasi_pupuk[0][1]:.2f} kg/Ha")
print(f"Pupuk KCl: {rekomendasi_pupuk[0][2]:.2f} kg/Ha")